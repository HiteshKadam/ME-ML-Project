{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63025520",
   "metadata": {},
   "source": [
    "# Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c132e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9df60b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input size of the images\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c971a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directories of the training and validation data\n",
    "train_data_dir = r\"./charts/train_val\"\n",
    "val_data_dir = r\"./charts/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7939ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the VGG16 model with pre-trained weights\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d784706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "513fdbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new layers to the pre-trained model\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0dc9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new model with the pre-trained model as its base and the new layers as its top\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4543bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a binary crossentropy loss and an Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4738cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation for the training data and validation data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba578b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Set the number of training and validation samples\n",
    "nb_train_samples = 1600\n",
    "nb_val_samples = 400\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fe22a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model with the data generators\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m     train_datagen\u001b[39m.\u001b[39;49mflow_from_directory(train_data_dir, target_size\u001b[39m=\u001b[39;49m(img_width, img_height), batch_size\u001b[39m=\u001b[39;49mbatch_size, class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbinary\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      4\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mnb_train_samples \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[0;32m      5\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      6\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_datagen\u001b[39m.\u001b[39;49mflow_from_directory(val_data_dir, target_size\u001b[39m=\u001b[39;49m(img_width, img_height), batch_size\u001b[39m=\u001b[39;49mbatch_size, class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbinary\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      7\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mnb_val_samples \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size)\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1108\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cluster_coordinator \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mcoordinator\u001b[39m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1103\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1105\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   1106\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1107\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1108\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1109\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1110\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1111\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1112\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1113\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1114\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   1115\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   1116\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1117\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   1118\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1119\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1120\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1121\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1122\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1124\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1348\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1347\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1348\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify_data_adapter_compatibility(adapter_cls)\n\u001b[1;32m-> 1138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1139\u001b[0m     x,\n\u001b[0;32m   1140\u001b[0m     y,\n\u001b[0;32m   1141\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1142\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1143\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1144\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1145\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1146\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1147\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1148\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1149\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1150\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1152\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:916\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keras_sequence \u001b[39m=\u001b[39m x\n\u001b[0;32m    915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enqueuer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m \u001b[39msuper\u001b[39m(KerasSequenceAdapter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    917\u001b[0m     x,\n\u001b[0;32m    918\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# Shuffle is handed in the _make_callable override.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     workers\u001b[39m=\u001b[39mworkers,\n\u001b[0;32m    920\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39muse_multiprocessing,\n\u001b[0;32m    921\u001b[0m     max_queue_size\u001b[39m=\u001b[39mmax_queue_size,\n\u001b[0;32m    922\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    923\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:793\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[39msuper\u001b[39m(GeneratorDataAdapter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    791\u001b[0m \u001b[39m# Since we have to know the dtype of the python generator when we build the\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[39m# dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m peek, x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_peek_and_restore(x)\n\u001b[0;32m    794\u001b[0m peek \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_standardize_batch(peek)\n\u001b[0;32m    795\u001b[0m peek \u001b[39m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:927\u001b[0m, in \u001b[0;36mKerasSequenceAdapter._peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    926\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_peek_and_restore\u001b[39m(x):\n\u001b[1;32m--> 927\u001b[0m   \u001b[39mreturn\u001b[39;00m x[\u001b[39m0\u001b[39;49m], x\n",
      "File \u001b[1;32mc:\\Users\\HiteshKadam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py:54\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 54\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAsked to retrieve element \u001b[39m\u001b[39m{idx}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     55\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mbut the Sequence \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     56\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mhas length \u001b[39m\u001b[39m{length}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx\u001b[39m=\u001b[39midx,\n\u001b[0;32m     57\u001b[0m                                                       length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)))\n\u001b[0;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m         np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[1;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "# Train the model with the data generators\n",
    "history = model.fit(\n",
    "    train_datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary'),\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_datagen.flow_from_directory(val_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary'),\n",
    "    validation_steps=nb_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_data_dir =r\"C:\\Users\\SANTANU\\Downloads\\test-20230423T074543Z-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 25s 6s/step - loss: 7.7675e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e1dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n",
      "Test loss: 0.0007767498027533293\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11655b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff319678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
